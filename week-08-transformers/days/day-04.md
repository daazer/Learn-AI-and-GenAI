# week-08-transformers - Day 04: Positional Encoding

## Reading (5-15 minutes)
- [Positional encoding explanation](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/) - skim formulas.
- [Transformer paper section 3.5](https://arxiv.org/abs/1706.03762) - positional encoding paragraph.

## How To Read (2-3 minutes)
- Write one reason attention-only models need position info.

## Build (Detailed Coding / Practice)
1. Create `week-08-transformers/code/day04_positional_encoding.py`.
2. Implement sinusoidal encoding and add it to token embeddings.
3. Save visualization plot to `artifacts/day-04-posenc.png`.

## Practice Drills
- Compare first 16 vs 64 positions for pattern repetition.

## Done When
- [ ] Sinusoidal encoding implemented and visualized.

