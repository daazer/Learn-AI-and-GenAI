# week-07-language-models - Day 04: Perplexity Evaluation

## Reading (5-15 minutes)
- [Perplexity definition](https://en.wikipedia.org/wiki/Perplexity) - LM section.
- [HF perplexity note](https://huggingface.co/docs/transformers/en/perplexity) - skim intro.

## How To Read (2-3 minutes)
- Write one sentence linking perplexity to uncertainty.

## Build (Detailed Coding / Practice)
1. Create `week-07-language-models/code/day04_perplexity.py`.
2. Evaluate unigram/bigram/bigram-smoothed on same dev text.
3. Save ranked results to `artifacts/day-04-perplexity.md`.

## Practice Drills
- Check tokenizer consistency across all models.

## Done When
- [ ] Perplexity comparison report exists.

